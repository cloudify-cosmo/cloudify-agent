#########
# Copyright (c) 2015 GigaSpaces Technologies Ltd. All rights reserved
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
#  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  * See the License for the specific language governing permissions and
#  * limitations under the License.

import getpass
import json
import logging
import os
import time

from pika.exceptions import ConnectionClosed, ProbableAuthenticationError

from cloudify.utils import (LocalCommandRunner,
                            setup_logger)
from cloudify import constants
from cloudify.amqp_client import get_client
from cloudify.exceptions import CommandExecutionException

from cloudify_agent import VIRTUALENV
from cloudify_agent.api import utils
from cloudify_agent.api import exceptions
from cloudify_agent.api import defaults


AGENT_IS_REGISTERED_TIMEOUT = 1


class Daemon(object):
    """Base class for daemon implementations.

    Following is all the available common daemon keyword arguments. These
    will be available to any daemon without any configuration as instance
    attributes.

    ``user``:

        the user this daemon will run under. default to the current user.

    ``name``:

        the name to give the daemon. This name will be a unique identifier of
        the daemon. meaning you will not be able to create more daemons with
        that name until a delete operation has been performed. defaults to
        a unique name generated by cloudify.

    ``queue``:

        the queue this daemon will listen to. It is possible to create
        different workers with the same queue, however this is discouraged.
        to create more workers that process tasks from a given queue, use the
        'min_workers' and 'max_workers' keys. defaults to <name>-queue.

    ``host``:

        the ip address of the host the agent will be started on. this
        property is used only when the 'queue' or 'name' property are omitted,
        in order to retrieve the agent name and queue from the manager. in
        such case, this property must match the 'ip' runtime property given
        to the corresponding Compute node.

    ``deployment_id``:

        the deployment id this agent will be a part of. this
        property is used only when the 'queue' or 'name' property are omitted,
        in order to retrieve the agent name and queue from the manager.

    ``workdir``:

        working directory for runtime files (pid, log).
        defaults to the current working directory.

    ``broker_ip``:

        the host name or ip address of the broker to connect to.

    ``broker_ssl_enabled``:

        Whether SSL is enabled for the broker.

    ``broker_ssl_cert``:

        The SSL public certificate for the broker, if SSL is enabled on the
        broker. This should be in PEM format and should be the string
        representation, including the 'BEGIN CERTIFICATE' header and footer.

    ``broker_user``

        the username for the broker connection
        defaults to 'guest'

    ``broker_pass``

        the password for the broker connection
        defaults to 'guest'

    ``heartbeat``

        The AMQP heartbeats interval to be used by agents,
        in seconds.
        Defaults to 30.

    ``rest_host``:

        the ip address/host name of the manager, running the
        REST service. (Required)

    ``rest_port``:

        the manager REST gateway port to connect to. defaults to 80.

    ``rest_token``:

        the token to use in REST calls. No default.

    ``local_rest_cert_file``:

        A path to a local copy of the manager's SSL certificate, to be used
        for certificate verification

    ``min_workers``:

        the minimum number of worker processes this daemon will manage. all
        workers will listen on the same queue allowing for higher
        concurrency when preforming tasks. defaults to 0.

    ``max_workers``:

        the maximum number of worker processes this daemon will manage.
        as tasks keep coming in, the daemon will expand its worker pool to
        handle more tasks concurrently. However, as the name
        suggests, it will never exceed this number. allowing for the control
        of resource usage. defaults to 5.

    ``log_level``:

        log level of the daemon process itself. defaults to debug.

    ``log_dir``:

        location of the directory to store daemon logs. defaults to workdir

    ``pid_file``:

        location of the daemon pid file. defaults to <workdir>/<name>.pid

    """

    # override this when adding implementations.
    PROCESS_MANAGEMENT = None

    def __init__(
        self,
        *,
        logger=None,
        broker_ip,
        name,
        queue,
        deployment_id=None,
        user=None,
        broker_user='guest',
        broker_pass='guest',
        broker_vhost='/',
        broker_ssl_enabled=False,
        broker_ssl_cert_path=None,
        heartbeat=None,
        min_workers=defaults.MIN_WORKERS,
        max_workers=defaults.MAX_WORKERS,
        agent_dir=None,
        local_rest_cert_file=None,
        log_max_bytes=defaults.LOG_FILE_SIZE,
        log_max_history=defaults.LOG_BACKUPS,
        executable_temp_path=None,
        extra_env=None,
        log_level=defaults.LOG_LEVEL,
        log_dir=None,
        pid_file=None,
        network='default',
        resources_root='/tmp/resources',
        **params
    ):
        self._logger = logger or setup_logger(
            logger_name='cloudify_agent.api.pm.{0}'
            .format(self.PROCESS_MANAGEMENT))

        self._runner = LocalCommandRunner(logger=self._logger)

        self.broker_ip = broker_ip

        self.name = name
        self.user = user or getpass.getuser()

        self.broker_user = broker_user
        self.broker_pass = broker_pass
        self.broker_vhost = broker_vhost
        self.broker_ssl_enabled = broker_ssl_enabled
        self.broker_ssl_cert_path = broker_ssl_cert_path
        if self.broker_ssl_enabled:
            self.broker_port = constants.BROKER_PORT_SSL
        else:
            self.broker_port = constants.BROKER_PORT_NO_SSL
        self.heartbeat = heartbeat

        self.deployment_id = deployment_id
        self.queue = queue

        self.min_workers = min_workers
        self.max_workers = max_workers

        self.agent_dir = agent_dir or os.getcwd()
        self.workdir = os.path.join(self.agent_dir, 'work')
        self.local_rest_cert_file = (
            local_rest_cert_file
            or os.path.join(
                self.agent_dir, 'cloudify', 'ssl', 'cloudify_internal_cert.pem'
            )
        )

        self.log_max_bytes = log_max_bytes
        self.log_max_history = log_max_history
        self.log_level = log_level
        self.log_dir = log_dir or self.workdir

        self.executable_temp_path = executable_temp_path
        self.extra_env = extra_env or {}

        # CentOS / RHEL 6 don't have /run; they have /var/run which is cleared
        # at boot time.
        # CentOS / RHEL 7 mount /run on tmpfs, and symlink /var/run to it.
        # We'll use /var/run globally for the time being.
        self.pid_file = (
            pid_file or '/var/run/cloudify.{0}/agent.pid'.format(self.name)
        )

        # create working directory if its missing
        if not os.path.exists(self.workdir):
            self._logger.debug('Creating directory: {0}'.format(self.workdir))
            os.makedirs(self.workdir)

        # save as attributes so that they will be persisted in the json files.
        # we will make use of these values when loading agents by name.
        self.process_management = self.PROCESS_MANAGEMENT
        self.virtualenv = VIRTUALENV
        self.network = network

        self.resources_root = resources_root or '/tmp/resources'

        if self._logger.isEnabledFor(logging.DEBUG):
            printed_params = self.as_dict()
            for hidden_field in ['broker_pass', 'service_password']:
                printed_params.pop(hidden_field, None)
            self._logger.debug("Daemon attributes: %s", json.dumps(
                printed_params, indent=4))

        self._validate_autoscale(min_workers, max_workers)

    def create_broker_conf(self):
        self._logger.info('Deploying broker configuration.')
        config = {
            'broker_ssl_enabled': self.broker_ssl_enabled,
            'broker_cert_path': self.broker_ssl_cert_path,
            'broker_username': self.broker_user,
            'broker_password': self.broker_pass,
            'broker_hostname': self.broker_ip,
            'broker_vhost': self.broker_vhost,
            'broker_heartbeat': self.heartbeat,
        }
        broker_conf_path = os.path.join(self.workdir, 'broker_config.json')
        with open(broker_conf_path, 'w') as conf_handle:
            json.dump(config, conf_handle)

    def as_dict(self):
        return {k: v for k, v in vars(self).items() if not k.startswith('_')}

    def _get_client(self):
        return get_client(
            self.broker_ip,
            self.broker_user,
            self.broker_pass,
            self.broker_port,
            self.broker_vhost,
            self.broker_ssl_enabled,
            self.broker_ssl_cert_path,
            name=self.name
        )

    def _is_daemon_running(self):
        self._logger.debug('Checking if agent daemon is running...')
        client = self._get_client()
        try:
            with client:
                # precreate the queue that the agent will use, so that the
                # message is already waiting for the agent when it starts up
                client.channel_method(
                    'queue_declare',
                    queue='{0}_service'.format(self.queue),
                    auto_delete=False,
                    durable=True)
                return utils.is_agent_alive(
                    self.queue, client, timeout=3, connect=False)
        except (ConnectionClosed, ProbableAuthenticationError):
            self._logger.debug('Connection error, AMQP is unreachable.')
            return False

    ########################################################################
    # the following methods must be implemented by the sub-classes as they
    # may exhibit custom logic. usually this would be related to process
    # management specific configuration files.
    ########################################################################

    def delete(self, force=defaults.DAEMON_FORCE_DELETE):

        """
        Delete any resources created for the daemon in the 'configure' method.

        :param force: if the daemon is still running, stop it before
                      deleting it.
        """
        raise NotImplementedError('Must be implemented by a subclass')

    def start_command(self):

        """
        Construct a command line for starting the daemon.
        (e.g sudo service <name> start)

        :return a one liner command to start the daemon process.
        """
        raise NotImplementedError('Must be implemented by a subclass')

    def stop_command(self):

        """
        Construct a command line for stopping the daemon.
        (e.g sudo service <name> stop)

        :return a one liner command to stop the daemon process.
        """
        raise NotImplementedError('Must be implemented by a subclass')

    def status(self):

        """
        Query the daemon status, This method can be usually implemented
        by simply running the status command. However, this is not always
        the case, as different commands and process management tools may
        behave differently.

        :return: True if the service is running, False otherwise
        """
        raise NotImplementedError('Must be implemented by a subclass')

    def create_script(self):
        raise NotImplementedError('Must be implemented by a subclass')

    def create_config(self):
        raise NotImplementedError('Must be implemented by a subclass')

    ########################################################################
    # the following methods is the common logic that would apply to any
    # process management implementation.
    ########################################################################

    def create(self):

        """
        Creates the agent. This method may be served as a hook to some custom
        logic that needs to be implemented after the instance
        was instantiated.

        """
        self._logger.debug('Daemon created')

    def configure(self):

        """
        Creates any necessary resources for the daemon. After this method
        was completed successfully, it should be possible to start the daemon
        by running the command returned by the `start_command` method.

        """
        self.create_script()
        self.create_config()

    def start(self,
              interval=defaults.START_INTERVAL,
              timeout=defaults.START_TIMEOUT,
              delete_amqp_queue=defaults.DELETE_AMQP_QUEUE_BEFORE_START):

        """
        Starts the daemon process.

        :param interval: the interval in seconds to sleep when waiting for
                         the daemon to be ready.
        :param timeout: the timeout in seconds to wait for the daemon to be
                        ready.
        :param delete_amqp_queue: delete any queues with the name of the
                                  current daemon queue in the broker.

        :raise DaemonStartupTimeout: in case the agent failed to start in the
        given amount of time.
        :raise DaemonException: in case an error happened during the agent
        startup.

        """
        if delete_amqp_queue:
            self._logger.warning('Deprecation warning:\n'
                                 'The `delete_amqp_queue` param is no '
                                 'longer used, and it is only left for '
                                 'backwards compatibility')
        start_command = self.start_command()
        self._logger.info('Starting daemon with command: {0}'
                          .format(start_command))
        self._runner.run(start_command)
        end_time = time.time() + timeout
        while time.time() < end_time:
            if self._is_daemon_running():
                return
            self._logger.debug('Daemon {0} is still not running. '
                               'Sleeping for {1} seconds...'
                               .format(self.name, interval))
            time.sleep(interval)
        self._verify_no_error()
        raise exceptions.DaemonStartupTimeout(timeout, self.name)

    def stop(self,
             interval=defaults.STOP_INTERVAL,
             timeout=defaults.STOP_TIMEOUT):

        """
        Stops the daemon process.

        :param interval: the interval in seconds to sleep when waiting for
                         the daemon to stop.
        :param timeout: the timeout in seconds to wait for the daemon to stop.

        :raise DaemonShutdownTimeout: in case the agent failed to be stopped
        in the given amount of time.
        :raise DaemonException: in case an error happened during the agent
        shutdown.

        """
        self.before_self_stop()

        stop_command = self.stop_command()
        self._logger.info('Stopping daemon with command: {0}'
                          .format(stop_command))
        self._runner.run(stop_command)
        end_time = time.time() + timeout
        while time.time() < end_time:
            self._logger.debug('Querying status of daemon {0}'.format(
                self.name))
            # make sure the status command also recognizes the
            # daemon is down
            status = self.status()
            if not status:
                self._logger.debug('Daemon {0} has shutdown'
                                   .format(self.name))
                return
            self._logger.debug('Daemon {0} is still running. '
                               'Sleeping for {1} seconds...'
                               .format(self.name, interval))
            time.sleep(interval)
        self._verify_no_error()
        raise exceptions.DaemonShutdownTimeout(timeout, self.name)

    def restart(self,
                start_timeout=defaults.START_TIMEOUT,
                start_interval=defaults.START_INTERVAL,
                stop_timeout=defaults.STOP_TIMEOUT,
                stop_interval=defaults.STOP_INTERVAL):

        """
        Restart the daemon process.

        :param start_interval: the interval in seconds to sleep when waiting
                               for the daemon to start.
        :param start_timeout: The timeout in seconds to wait for the daemon
                              to start.
        :param stop_interval: the interval in seconds to sleep when waiting
                              for the daemon to stop.
        :param stop_timeout: the timeout in seconds to wait for the daemon
                             to stop.

        :raise DaemonStartupTimeout: in case the agent failed to start in the
        given amount of time.
        :raise DaemonShutdownTimeout: in case the agent failed to be stopped
        in the given amount of time.
        :raise DaemonException: in case an error happened during startup or
        shutdown

        """

        self.stop(timeout=stop_timeout,
                  interval=stop_interval)
        self.start(timeout=start_timeout,
                   interval=start_interval)

    def before_self_stop(self):
        """Called before stopping the daemon.

        Subclasses can implement this for cleanup.
        """

    def _verify_no_error(self):
        error_dump_path = os.path.join(
            utils.internal.get_storage_directory(self.user),
            '{0}.err'.format(self.name))

        # this means the agent worker had an uncaught
        # exception and it wrote its content
        # to the file above because of our custom exception
        # handler (see worker.py)
        if os.path.exists(error_dump_path):
            with open(error_dump_path) as f:
                error = f.read()
            os.remove(error_dump_path)
            raise exceptions.DaemonError(error)

    def _validate_autoscale(self, min_workers, max_workers):
        if min_workers:
            if not str(min_workers).isdigit():
                raise exceptions.DaemonPropertiesError(
                    'min_workers is supposed to be a number '
                    'but is: {0}'
                    .format(min_workers)
                )
            min_workers = int(min_workers)
        if max_workers:
            if not str(max_workers).isdigit():
                raise exceptions.DaemonPropertiesError(
                    'max_workers is supposed to be a number '
                    'but is: {0}'
                    .format(max_workers)
                )
            max_workers = int(max_workers)
        if min_workers and max_workers:
            if min_workers > max_workers:
                raise exceptions.DaemonPropertiesError(
                    'min_workers cannot be greater than max_workers '
                    '[min_workers={0}, max_workers={1}]'
                    .format(min_workers, max_workers))


class GenericLinuxDaemonMixin(Daemon):
    SCRIPT_DIR = None
    CONFIG_DIR = None

    def status_command(self):
        raise NotImplementedError('Must be implemented by a subclass')

    def _delete(self):
        raise NotImplementedError('Must be implemented by a subclass')

    def _get_rendered_script(self):
        raise NotImplementedError('Must be implemented by a subclass')

    def _get_rendered_config(self):
        raise NotImplementedError('Must be implemented by a subclass')

    def _get_script_path(self):
        raise NotImplementedError('Must be implemented by a subclass')

    def __init__(self, logger=None, **params):
        super(GenericLinuxDaemonMixin, self).__init__(logger=logger, **params)

        # CY-1852: When reading a daemon, 'service_name' is guaranteed to be
        # there. When creating a daemon, 'service_name' isn't there - it's
        # supposed to be set in this function.
        self.service_name = params.get('service_name')
        if not self.service_name:
            self.service_name = 'cloudify-worker-{0}'.format(self.name)
        self.script_path = self._get_script_path()
        self.config_path = os.path.join(self.CONFIG_DIR, self.service_name)

    def status(self):
        try:
            self._runner.run(self.status_command())
            return True
        except CommandExecutionException as e:
            self._logger.debug('%s', e)
            return False

    def create_script(self):
        rendered = self._get_rendered_script()
        self._runner.run('sudo mkdir -p {0}'.format(
            os.path.dirname(self.script_path)))
        self._runner.run(
            'sudo cp {0} {1}'.format(rendered, self.script_path))
        self._runner.run('sudo rm {0}'.format(rendered))

    def create_config(self):
        rendered = self._get_rendered_config()
        self._runner.run('sudo mkdir -p {0}'.format(
            os.path.dirname(self.config_path)))
        self._runner.run('sudo cp {0} {1}'.format(rendered, self.config_path))
        self._runner.run('sudo rm {0}'.format(rendered))

    def delete(self, force=defaults.DAEMON_FORCE_DELETE):
        try:
            self.stop()
        except Exception as e:
            self._logger.info('Deleting agent: could not stop daemon: %s', e)

        self._delete()

        if os.path.exists(self.script_path):
            self._logger.debug('Deleting {0}'.format(self.script_path))
            self._runner.run('sudo rm {0}'.format(self.script_path))
        if os.path.exists(self.config_path):
            self._logger.debug('Deleting {0}'.format(self.config_path))
            self._runner.run('sudo rm {0}'.format(self.config_path))


class CronRespawnDaemonMixin(Daemon):

    """
    This Mixin exposes capabilities for adding a cron job that re-spawns
    the daemon in case of a failure.

    Usage:

        run(self.create_enable_cron_script)
        run(self.create_disable_cron_script)

    Following are all possible custom key-word arguments
    (in addition to the ones available in the base daemon)

    ``cron_respawn``

        pass True to enable cron detection. False otherwise

    ``cron_respawn_delay``

        the amount of minutes to wait before each cron invocation.

    """

    def __init__(self, logger=None, **params):
        super().__init__(logger=logger, **params)
        self.cron_respawn_delay = params.get('cron_respawn_delay', 1)
        self.cron_respawn = params.get('cron_respawn', False)

    def status_command(self):

        """
        Construct a command line for querying the status of the daemon.
        (e.g sudo service <name> status)
        the command execution should result in a zero return code if the
        service is running, and a non-zero return code otherwise.

        :return a one liner command to start the daemon process.
        :rtype: str
        """
        raise NotImplementedError('Must be implemented by a subclass')

    @property
    def cron_respawn_path(self):
        return os.path.join(
            self.workdir, '{0}-respawn.sh'.format(self.name))

    def create_enable_cron_script(self):
        enable_cron_script = os.path.join(
            self.workdir, '{0}-enable-cron.sh'.format(self.name))

        self._logger.debug('Rendering respawn script from template')
        utils.render_template_to_file(
            template_path='respawn.sh.template',
            file_path=self.cron_respawn_path,
            start_command=self.start_command(),
            status_command=self.status_command()
        )
        self._runner.run('chmod +x {0}'.format(self.cron_respawn_path))
        self._logger.debug('Rendering enable cron script from template')
        utils.render_template_to_file(
            template_path='crontab/enable.sh.template',
            file_path=enable_cron_script,
            cron_respawn_delay=self.cron_respawn_delay,
            cron_respawn_path=self.cron_respawn_path,
            workdir=self.workdir,
            name=self.name
        )
        self._runner.run('chmod +x {0}'.format(enable_cron_script))
        return enable_cron_script

    def create_disable_cron_script(self):
        disable_cron_script = os.path.join(
            self.workdir, '{0}-disable-cron.sh'.format(self.name))

        self._logger.debug('Rendering disable cron script from template')
        utils.render_template_to_file(
            template_path='crontab/disable.sh.template',
            file_path=disable_cron_script,
            name=self.name,
            workdir=self.workdir
        )
        self._runner.run('chmod +x {0}'.format(disable_cron_script))
        return disable_cron_script
